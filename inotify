import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hdfs.DFSInotifyEventInputStream
import org.apache.hadoop.hdfs.client.HdfsAdmin
import org.apache.hadoop.hdfs.inotify.{Event, EventBatch}
import java.net.URI
import scala.util.{Failure, Success, Try}

object HDFSInotifyMonitor {

  // =========================== CONFIGURATION =========================== //
  val hdfsUri = "hdfs://namenode:8020"
  val monitorPath = "/user/hive/warehouse/input_data"
  val pollIntervalMillis: Long = 1000
  val maxRunDurationMillis: Long = 24 * 60 * 60 * 1000 // 24 hours in milliseconds

  // =========================== PROCESS HDFS EVENT =========================== //
  def processHdfsEvent(event: Event): Unit = {
    event match {
      case createEvent: Event.Create =>
        println(s"File created: ${createEvent.getPath}")
        // Trigger Spark job or ETL pipeline
      case appendEvent: Event.Append =>
        println(s"Data appended to: ${appendEvent.getPath}")
      case closeEvent: Event.Close =>
        println(s"File closed: ${closeEvent.getPath}")
      case renameEvent: Event.Rename =>
        println(s"File renamed from ${renameEvent.getSrcPath} to ${renameEvent.getDstPath}")
      case deleteEvent: Event.Delete =>
        println(s"File deleted: ${deleteEvent.getPath}")
      case _ =>
        println("Unknown event type")
    }
  }

  // =========================== MONITOR HDFS PATH =========================== //
  def monitorHdfsPath(): Unit = {
    val conf = new Configuration()
    val hdfsAdmin = new HdfsAdmin(new URI(hdfsUri), conf)
    val eventStream: DFSInotifyEventInputStream = hdfsAdmin.getInotifyEventStream

    // Get the latest transaction ID
    val startTxId = eventStream.getCurrentTxId
    println(s"Starting from transaction ID: $startTxId")

    // Record start time
    val startTime = System.currentTimeMillis()

    // Loop until duration exceeds defined time limit
    while ((System.currentTimeMillis() - startTime) < maxRunDurationMillis) {
      Try(eventStream.poll(pollIntervalMillis)) match {
        case Success(eventBatch: EventBatch) if eventBatch != null =>
          println(s"Processing batch with TxID: ${eventBatch.getTxid}")
          eventBatch.getEvents.foreach(processHdfsEvent)
        case Failure(exception) =>
          println(s"Error while processing events: ${exception.getMessage}")
          Thread.sleep(2000) // Sleep before retry
        case _ =>
          // Sleep briefly to avoid CPU overuse when no events occur
          Thread.sleep(500)
      }
    }

    // =========================== CLEAN UP =========================== //
    println("Job execution time exceeded, exiting gracefully...")
    closeResources(eventStream)
  }

  // =========================== CLEAN UP RESOURCES =========================== //
  def closeResources(eventStream: DFSInotifyEventInputStream): Unit = {
    if (eventStream != null) {
      try {
        eventStream.close()
        println("Event stream closed successfully.")
      } catch {
        case e: Exception =>
          println(s"Error closing event stream: ${e.getMessage}")
      }
    }
  }

  // =========================== MAIN METHOD =========================== //
  def main(args: Array[String]): Unit = {
    println("Starting HDFS Inotify Monitor...")
    monitorHdfsPath()
  }
}


import org.scalatest.funsuite.AnyFunSuite
import org.scalatest.matchers.should.Matchers
import org.apache.hadoop.hdfs.inotify.Event

class HDFSInotifyMonitorTest extends AnyFunSuite with Matchers {

  test("processHdfsEvent should handle create event correctly") {
    val createEvent = new Event.Create("/path/to/file", false, "", 0, 0, null, null)
    val output = captureOutput {
      HDFSInotifyMonitor.processHdfsEvent(createEvent)
    }
    output should include("File created: /path/to/file")
  }

  test("monitorHdfsPath should exit after configured duration") {
    val output = captureOutput {
      HDFSInotifyMonitor.main(Array())
    }
    output should include("Job execution time exceeded, exiting gracefully...")
  }

  // Helper method to capture console output
  def captureOutput(block: => Unit): String = {
    val stream = new java.io.ByteArrayOutputStream()
    Console.withOut(stream)(block)
    stream.toString
  }
}

==================================================================

import akka.actor.{Actor, ActorRef, ActorSystem, Props}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}
import scala.concurrent.duration._

case class CheckDirectory()
case class FileDetected(filePath: String)

// =========================
// 1. FileWatcher Actor
// =========================
class FileWatcherActor(directoryPath: String, workerActor: ActorRef) extends Actor {
  val hadoopConf = new Configuration()
  val fs: FileSystem = FileSystem.get(hadoopConf)
  var lastSeenFiles: Set[String] = Set()

  import context.dispatcher
  // Schedule to run check every 10 seconds
  context.system.scheduler.scheduleWithFixedDelay(0.seconds, 10.seconds, self, CheckDirectory())

  def receive: Receive = {
    case CheckDirectory() =>
      val path = new Path(directoryPath)
      if (fs.exists(path)) {
        val fileStatus: Array[FileStatus] = fs.listStatus(path)
        val currentFiles = fileStatus.map(_.getPath.toString).toSet

        // Detect new files
        val newFiles = currentFiles.diff(lastSeenFiles)
        newFiles.foreach { file =>
          println(s"New file detected: $file")
          workerActor ! FileDetected(file)
        }

        // Update the last seen file list
        lastSeenFiles = currentFiles
      } else {
        println(s"Directory does not exist: $directoryPath")
      }
  }
}

// =========================
// 2. Worker Actor (File Processor)
// =========================
class FileProcessorActor extends Actor {
  def receive: Receive = {
    case FileDetected(filePath) =>
      println(s"Processing file: $filePath")
      // Process the file (e.g., Spark job or ETL)
      processFile(filePath)
  }

  def processFile(filePath: String): Unit = {
    // Implement the file processing logic here
    println(s"File processed successfully: $filePath")
  }
}

// =========================
// 3. Main Application to Start Actors
// =========================
object HdfsFileMonitorApp extends App {
  val system: ActorSystem = ActorSystem("HdfsFileMonitorSystem")

  val workerActor: ActorRef = system.actorOf(Props[FileProcessorActor], "fileProcessorActor")
  val directoryPath = "hdfs://namenode:8020/user/hdfs/input"

  val fileWatcherActor: ActorRef = system.actorOf(Props(new FileWatcherActor(directoryPath, workerActor)), "fileWatcherActor")
}


libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-actor" % "2.6.20",
  "com.typesafe.akka" %% "akka-testkit" % "2.6.20" % Test,
  "org.scalatest" %% "scalatest" % "3.2.14" % Test,
  "org.apache.hadoop" % "hadoop-common" % "3.2.2"
)

import akka.actor.{ActorRef, ActorSystem, Props}
import akka.testkit.{ImplicitSender, TestActorRef, TestKit}
import org.scalatest.BeforeAndAfterAll
import org.scalatest.wordspec.AnyWordSpecLike

class HdfsFileMonitorTest
    extends TestKit(ActorSystem("HdfsFileMonitorTestSystem"))
    with ImplicitSender
    with AnyWordSpecLike
    with BeforeAndAfterAll {

  override def afterAll(): Unit = {
    TestKit.shutdownActorSystem(system)
  }

  "FileWatcherActor" should {
    "detect new files and notify FileProcessorActor" in {
      val workerActor: ActorRef = TestActorRef(new FileProcessorActor)
      val directoryPath = "hdfs://namenode:8020/user/hdfs/input"

      val fileWatcherActor = TestActorRef(new FileWatcherActor(directoryPath, workerActor))
      fileWatcherActor ! CheckDirectory()

      expectNoMessage()
    }
  }

  "FileProcessorActor" should {
    "process the detected file correctly" in {
      val fileProcessorActor = TestActorRef(new FileProcessorActor)
      val filePath = "hdfs://namenode:8020/user/hdfs/input/test-file.txt"

      fileProcessorActor ! FileDetected(filePath)
      expectNoMessage()
    }
  }
}
